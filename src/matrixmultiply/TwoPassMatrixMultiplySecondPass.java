package matrixmultiply;
import java.io.IOException;
import java.util.HashMap;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;


public class TwoPassMatrixMultiplySecondPass 
{
	public static class Map 
	extends Mapper<LongWritable,Text,Text,Text>
	{
		private boolean debug = false;
		
		/**
		 * The Hadoop Map-Reduce framework spawns one map
		 * task for each InputSplit generated by the InputFormat 
		 * for this job. The framework first calls setup, 
		 * followed by map for each key/value pair in the 
		 * InputSplit
		 */
		public void setup(Context context)
		{
			Configuration conf = context.getConfiguration();
			debug = Boolean.parseBoolean(conf.get("debug"));			
			if (debug)
			{
				System.out.println("Starting pass 2");
			}
		}
		
		public void map(LongWritable key,
				Text val,Context context) throws IOException, InterruptedException
		{		
			if (debug)
			{				
				System.out.println("Map input: Key " + key + ",Value " +  val);
			}
			try
			{
				context.write(new Text(key.toString()),new Text(val));
			}
			catch(Exception ex)
			{
				ex.printStackTrace();
			}
		}
	}
	
	public static class Reduce extends Reducer<Text,
	Text,MatrixMultiply.IndexPair,DoubleWritable>
	{		
		boolean debug;
		java.util.Map<String,Double> pik = new HashMap<String,Double>(); 
				
		public void setup(Context context)
		{
			Configuration conf = context.getConfiguration();
			debug = Boolean.parseBoolean(conf.get("debug"));			
		}
		
		public void reduce(Text key,
				Iterable<Text> values,Context context) throws IOException, InterruptedException
		{
			//sum over the same set of keys
			for (Text value : values)
			{
				String[] indicesAndValue = value.toString().split(",");
				String k = indicesAndValue[0] + "," + indicesAndValue[1];
				double sum = 0.0;
				if (pik.get(k) == null)
				{
					sum = 0.0;					
				}
				else
				{
					sum = pik.get(k);
				}		
				sum += Double.parseDouble(indicesAndValue[2]);
				
				pik.put(k, sum);
			}
			//write the matrix product to file	
			for (String k : pik.keySet())
			{
				MatrixMultiply.IndexPair indexPair = new MatrixMultiply.IndexPair();
				String indices[] = k.split(",");
				indexPair.row = Integer.parseInt(indices[0]);
				indexPair.col = Integer.parseInt(indices[1]);
				DoubleWritable res = new DoubleWritable();
				res.set(Double.parseDouble(pik.get(k).toString()));
				context.write(indexPair, res);
			}		
		}
	}
	
	@SuppressWarnings("deprecation")
	public static void run(Configuration conf) throws Exception
	{
		Job pass2 = new Job(conf,"Matrix multiplication pass 2 of 2");
		pass2.setJarByClass(TwoPassMatrixMultiplySecondPass.class);
		pass2.setInputFormatClass(TextInputFormat.class);
		pass2.setOutputFormatClass(SequenceFileOutputFormat.class);
		pass2.setMapperClass(Map.class);
		pass2.setReducerClass(Reduce.class);
		pass2.setMapOutputKeyClass(Text.class);
		pass2.setMapOutputValueClass(Text.class);
		pass2.setOutputKeyClass(MatrixMultiply.IndexPair.class);
		pass2.setOutputValueClass(DoubleWritable.class);
		//The input file location
		FileInputFormat.addInputPath(pass2, new Path(conf.get("tempPath")));
			
		//The output files location
		//make sure you delete any previous
		//setup
		FileSystem fs = FileSystem.get(conf);
		fs.delete(new Path(conf.get("outputPath")));
		FileOutputFormat.setOutputPath(pass2, new Path(conf.get("outputPath")));
		boolean pass2Success = pass2.waitForCompletion(true);
		if (!pass2Success)
		{
			throw new Exception("Pass 2 of 2 failed");
		}
	}
}

