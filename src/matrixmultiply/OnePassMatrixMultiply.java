package matrixmultiply;
import java.io.IOException;
import java.util.HashMap;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;


public class OnePassMatrixMultiply 
{
	public static class Map 
	extends Mapper<MatrixMultiply.IndexPair, 
	DoubleWritable,Text,Text>
	{
		private boolean matrixM;//are we mapping matrix M or N
		private Path path;
		private String inputPathM,inputPathN;
		private int I,J;
		private boolean debug = false;
		
		/**
		 * The Hadoop Map-Reduce framework spawns one map
		 * task for each InputSplit generated by the InputFormat 
		 * for this job. The framework first calls setup, 
		 * followed by map for each key/value pair in the 
		 * InputSplit
		 */
		public void setup(Context context)
		{
			Configuration conf = context.getConfiguration();
			inputPathM = conf.get("inputPathM");
			inputPathN = conf.get("inputPathN");
			debug = Boolean.parseBoolean(conf.get("debug"));
			FileSplit split = (FileSplit)context.getInputSplit();
			path = split.getPath();
			matrixM = path.toString().contains(inputPathM);
			I = Integer.parseInt(conf.get("I"));
			J = Integer.parseInt(conf.get("J"));
			if (debug)
			{
				System.out.println("Is matrix M " + matrixM + " for "+ path);
			}
		}
		
		public void map(MatrixMultiply.IndexPair key,
				DoubleWritable val,Context context) throws IOException, InterruptedException
		{
			int i,j,k;
			double value = val.get();
			Text outputKey = new Text();
			Text outputValue = new Text();
			
			if (debug)
			{				
				System.out.println(matrixM ? "M matrix ":"N matrix");
				System.out.println("Map input: (" + key.row + "," + key.col + ") " +  val.get());
			}
			if (matrixM)
			{
				//for as many columns of N
				for (j=0;j<J;j++)
				{
					i = key.row;
					k = key.col;
					outputKey.set(i + "," + j);
					outputValue.set("M," + k + "," + value);
					context.write(outputKey,outputValue);
					if (debug)
					{
						System.out.println(matrixM ? "M matrix":"N matrix");
						System.out.println("Map output:key=" +outputKey+" value="+outputValue);
					}
				}				
			}
			else
			{
				//for as many rows of M
				for (i=0;i<I;i++) 
				{
					k = key.row;
					j = key.col;
					outputKey.set(i + "," + j);
					outputValue.set("N," + k + "," + value);
					context.write(outputKey,outputValue);
					if (debug)
					{
						System.out.println(matrixM ? "M matrix":"N matrix");
						System.out.println("Map output:key=" +outputKey+" value="+outputValue);
					}
				}
			}
		}
	}
	
	public static class Reduce extends Reducer<Text,
	Text,MatrixMultiply.IndexPair,DoubleWritable>
	{		
		boolean debug;
		int K;
		
		public void setup(Context context)
		{
			Configuration conf = context.getConfiguration();
			debug = Boolean.parseBoolean(conf.get("debug"));			
			K = Integer.parseInt(conf.get("K"));
		}
		
		public void reduce(Text key,
				Iterable<Text> values,Context context) throws IOException, InterruptedException
		{
			double mik,nkj, pij;
			String[] value;
			java.util.Map<Integer,Double> mapM
			= new HashMap<Integer,Double>();
			java.util.Map<Integer,Double> mapN
			= new HashMap<Integer,Double>();
			for (Text val : values)
			{
				value = val.toString().split(",");
				if (value[0].equals("M"))
				{
					mapM.put(Integer.parseInt(value[1]),Double.parseDouble(value[2]));					
				}
				else
				{
					mapN.put(Integer.parseInt(value[1]),Double.parseDouble(value[2]));										
				}
			}
			if (debug)
			{
				System.out.println("Reduce M map" + mapM);
				System.out.println("Reduce N map" + mapN);
			}
			pij = 0.0;
			for (int k=0; k<K;k++)
			{
				mik = mapM.containsKey(k) ? mapM.get(k) : 0.0;
				nkj = mapN.containsKey(k) ? mapN.get(k) : 0.0;
				pij += mik * nkj;
			}
			
			if (pij != 0.0)
			{
				MatrixMultiply.IndexPair indexPair = new MatrixMultiply.IndexPair();
				String indices[] = key.toString().split(",");
				indexPair.row = Integer.parseInt(indices[0]);
				indexPair.col = Integer.parseInt(indices[1]);
				DoubleWritable res = new DoubleWritable();
				res.set(pij);
			
				try
				{
					context.write(indexPair, res);
					if (debug)
					{
						System.out.println("Reduce output: i,j= " + key + "pij=" + res);
					}
				}
				catch (Exception ex)
				{
					ex.printStackTrace();
				}
			}
		}
	}

	
	@SuppressWarnings("deprecation")
	public static void run(Configuration conf) throws Exception
	{
		Job pass1 = new Job(conf,"Matrix multiplication pass 1 of 1");
		pass1.setJarByClass(OnePassMatrixMultiply.class);
		pass1.setInputFormatClass(SequenceFileInputFormat.class);
		pass1.setOutputFormatClass(SequenceFileOutputFormat.class);
		pass1.setMapperClass(Map.class);
		pass1.setReducerClass(Reduce.class);
		pass1.setMapOutputKeyClass(Text.class);
		pass1.setMapOutputValueClass(Text.class);
		pass1.setOutputKeyClass(MatrixMultiply.IndexPair.class);
		pass1.setOutputValueClass(DoubleWritable.class);
		
		//The input files location
		FileInputFormat.addInputPath(pass1, new Path(conf.get("inputPathM")));
		FileInputFormat.addInputPath(pass1, new Path(conf.get("inputPathN")));
			
		//The output files location make sure you delete 
		//any previous setup
		FileSystem fs = FileSystem.get(conf);
		fs.delete(new Path(conf.get("outputPath")));
		FileOutputFormat.setOutputPath(pass1, new Path(conf.get("outputPath")));
		boolean passSuccess = pass1.waitForCompletion(true);
		if (!passSuccess)
		{
			throw new Exception("Pass 1 of 1 failed");
		}
	}
}
